{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Volume 3: Web Scraping 2</h1>\n",
    "    <Name>\n",
    "    <Class>\n",
    "    <Date>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "%matplotlib inline\n",
    "# rcParams[\"figure.figsize\"] = (16,12)    # Use this line to increase your figure size (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Modify `wunder_temp()` (below) so that it gathers the Actual Mean Temperature, Actual Max Temperature, and Actual Min Temperature for every day in July of 2012.\n",
    "Plot these three measurements against time on the same plot.\n",
    "Consider printing information at each iteration of the outer loop to keep track of the program’s progress.\n",
    "\n",
    "Display the plot inline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "def wunder_temp(day=\"/history/airport/KSAN/2012/7/1/DailyHistory.html\"):\n",
    "    \"\"\"Crawl through Weather Underground and extract temperature data.\"\"\"\n",
    "    # Initialize variables, including a regex for finding the 'Next Day' link.\n",
    "    actual_mean_temp = []\n",
    "    next_day_finder = re.compile(r\"Next Day\")\n",
    "    base_url = \"https://www.wunderground.com\"       # Web page base URL.\n",
    "    page = base_url + day                           # Complete page URL.\n",
    "    current = None\n",
    "\n",
    "    for _ in range(4):\n",
    "        while current is None:  # Try downloading until it works.\n",
    "            # Download the page source and PAUSE before continuing.\n",
    "            page_source = requests.get(page).text\n",
    "            time.sleep(1)           # PAUSE before continuing.\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            current = soup.find(string=\"Mean Temperature\")\n",
    "\n",
    "        # Navigate to the relevant tag, then extract the temperature data.\n",
    "        temp_tag = current.parent.parent.next_sibling.next_sibling.span.span\n",
    "        actual_mean_temp.append(int(temp_tag.string))\n",
    "\n",
    "        # Find the URL for the page with the next day's data.\n",
    "        new_day = soup.find(string=next_day_finder).parent[\"href\"]\n",
    "        page = base_url + new_day                   # New complete page URL.\n",
    "        current = None\n",
    "\n",
    "    return actual_mean_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Modify `bank_data()` so that it extracts the total consolidated assets (\"Consol Assets\") for JPMorgan Chase, Bank of America, and Wells Fargo recorded each December from 2004 to the present.\n",
    "In a single figure, plot each bank’s assets against time.\n",
    "Be careful to keep the data sorted by date.\n",
    "\n",
    "Display the plot inline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "def bank_data():\n",
    "    \"\"\"Crawl through the Federal Reserve site and extract bank data.\"\"\"\n",
    "    # Compile regular expressions for finding certain tags.\n",
    "    link_finder = re.compile(r\"2004$\")\n",
    "    chase_bank_finder = re.compile(r\"^JPMORGAN CHASE BK\")\n",
    "\n",
    "    # Get the base page and find the URLs to all other relevant pages.\n",
    "    base_url=\"https://www.federalreserve.gov/releases/lbr/\"\n",
    "    base_page_source = requests.get(base_url).text\n",
    "    base_soup = BeautifulSoup(base_page_source, \"html.parser\")\n",
    "    link_tags = base_soup.find_all(name='a', href=True, string=link_finder)\n",
    "    pages = [base_url + tag.attrs[\"href\"] for tag in link_tags]\n",
    "\n",
    "    # Crawl through the individual pages and record the data.\n",
    "    chase_assets = []\n",
    "    for page in pages:\n",
    "        time.sleep(1)               # PAUSE, then request the page.\n",
    "        soup = BeautifulSoup(requests.get(page).text, \"html.parser\")\n",
    "\n",
    "        # Find the tag corresponding to Chase Banks's consolidated assets.\n",
    "        temp_tag = soup.find(name=\"td\", string=chase_bank_finder)\n",
    "        for _ in range(10):\n",
    "            temp_tag = temp_tag.next_sibling\n",
    "        # Extract the data, removing commas.\n",
    "        chase_assets.append(int(temp_tag.string.replace(',', '')))\n",
    "\n",
    "    return chase_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "ESPN hosts data on NBA athletes at http://www.espn.go.com/nba/statistics.\n",
    "Each player has their own page with detailed performance statistics.\n",
    "For each of the five offensive leaders in points and each of the five defensive leaders in rebounds, extract the player's career minutes per game (MPG) and career points per game (PPG).\n",
    "Make a scatter plot of MPG against PPG for these ten players.\n",
    "\n",
    "Display the plot inline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "The arXiv (pronounced \"archive\") is an online repository of scientific publications, hosted by Cornell University.\n",
    "Write a function that accepts a string to serve as a search query.\n",
    "Use Selenium to enter the query into the search bar of https://arxiv.org and press Enter.\n",
    "The resulting page has up to 25 links to the PDFs of technical papers that match the query.\n",
    "Gather these URLs, then continue to the next page (if there are more results) and continue gathering links until obtaining at most 100 URLs.\n",
    "\n",
    "Print the list of URLs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Project Euler (https://projecteuler.net) is a collection of mathematical computing problems.\n",
    "Each problem is listed with an ID, a description/title, and the number of users that have solved the problem.\n",
    "\n",
    "Using Selenium, BeautifulSoup, or both, for each of the (at least) 600 problems in the archive at https://projecteuler.net/archives, record the problem ID and the number of people who have solved it.\n",
    "Return a list of IDs, sorted from largest to smallest by the number of people who have solved them.\n",
    "That is, the first entry in the list should be the ID of the most solved problem, and the last entry in the list should be the ID of the least solved problem.\n",
    "\n",
    "Print the list of problems IDs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Problem 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
